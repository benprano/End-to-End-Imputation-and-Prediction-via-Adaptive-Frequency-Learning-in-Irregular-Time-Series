{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2221e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from typing import Dict\n",
    "import pdb\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"AIR_QUALITY/\"\n",
    "csv_files = glob.glob(f\"{dataset_path}*.csv\")\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bef3dd",
   "metadata": {},
   "source": [
    "year: year of data in this row  , month: month of data in this row \n",
    "\n",
    "day: day of data in this row , hour: hour of data in this row \n",
    "\n",
    "PM2.5: PM2.5 concentration (ug/m^3) , PM10: PM10 concentration (ug/m^3)\n",
    "\n",
    "SO2: SO2 concentration (ug/m^3), NO2: NO2 concentration (ug/m^3)\n",
    "\n",
    "CO: CO concentration (ug/m^3),  O3: O3 concentration (ug/m^3)\n",
    "\n",
    "TEMP: temperature (degree Celsius) ,  PRES: pressure (hPa)\n",
    "\n",
    "DEWP: dew point temperature (degree Celsius),  RAIN: precipitation (mm)\n",
    "\n",
    "wd: wind direction,  WSPM: wind speed (m/s), station: name of the air-quality monitoring site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_all= ['No','year','month','day','hour','PM2.5', 'PM10','SO2','NO2','CO','O3','TEMP','PRES',\n",
    "           'DEWP','RAIN','wd','WSPM','station']\n",
    "keep_cols= ['PM2.5','PM10','SO2','NO2','CO','O3', 'TEMP','PRES','DEWP','RAIN','WSPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeca73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([pd.read_csv(f)[keep_cols] for f in glob.glob(f\"{dataset_path}*.csv\")], \n",
    "                      keys = cols_all*len(csv_files),ignore_index = True, axis=1,).set_axis(labels=keep_cols*len(csv_files),axis=1)\n",
    "\n",
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fc5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in glob.glob(f\"{dataset_path}*.csv\")], ignore_index = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d743ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb39f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dateInt']=df['year'].astype(str) + df['month'].astype(str).str.zfill(2)+ df['day'].astype(str).str.zfill(2)+df['hour'].astype(str).str.zfill(2)\n",
    "df['Date'] = pd.to_datetime(df['dateInt'], format='%Y%m%d%H')\n",
    "df['dateInt']=df['year'].astype(str) + df['month'].astype(str).str.zfill(2)+ df['day'].astype(str).str.zfill(2)\n",
    "df['date'] = pd.to_datetime(df['dateInt'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21533f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f988b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your dataframe and datetime_column is the column containing datetime values\n",
    "df['week_day'] = df['Date'].dt.day_name()\n",
    "# If you want the day as an integer (Monday = 0, Sunday = 6)\n",
    "df['week_day_numeric'] = df['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67227f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df =  df[(df['date'] >= '2013-03-01') & (df['date'] <= '2014-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecesary columns\n",
    "cols_to_drop = [\"No\", 'year', \"dateInt\", \"date\"]\n",
    "filtered_df = filtered_df.drop(cols_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1147b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ad81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wind_direction_to_trigonometric function\n",
    "def wind_direction_to_trigonometric(wind_direction):\n",
    "    angles = {\n",
    "        'WNW': 292.5, 'NW': 315, 'NE': 45, 'NNE': 22.5,\n",
    "        'ENE': 67.5, 'N': 0, 'ESE': 112.5, 'NNW': 337.5,\n",
    "        'E': 90, 'WSW': 247.5, 'W': 270, 'SW': 225,\n",
    "        'SSW': 202.5, 'SE': 135, 'S': 180, 'SSE': 157.5,\n",
    "        'NbE': 11.25, 'NEbN': 33.75, 'NEbE': 56.25, 'EbN': 78.75,\n",
    "        'EbS': 101.25, 'SEbE': 123.75, 'SEbS': 146.25, 'EbN': 78.75,\n",
    "    }\n",
    "    \n",
    "    if wind_direction in angles:\n",
    "        degrees = angles[wind_direction]\n",
    "        radians = np.radians(degrees)\n",
    "        return np.cos(radians), np.sin(radians)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[['cos_wind_direction', 'sin_wind_direction']] = filtered_df['wd'].apply(wind_direction_to_trigonometric).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025861c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET =\"PM2.5\"\n",
    "keep_cols = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP',\n",
    "            'RAIN', 'WSPM','cos_wind_direction', 'sin_wind_direction',\n",
    "            'month', \"hour\", 'week_day_numeric',TARGET]\n",
    "cols = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP',\n",
    "        'RAIN', 'WSPM','cos_wind_direction', 'sin_wind_direction',\n",
    "        'month', \"hour\", 'week_day_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887afb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data = filtered_df[keep_cols]\n",
    "air_quality_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b596d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f32d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 24\n",
    "prediction_horizon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766df7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_imputed = np.zeros((len(air_quality_data), timesteps, air_quality_data.shape[1]-1))\n",
    "X_data_no_imputed = np.zeros((len(air_quality_data), timesteps, air_quality_data.shape[1]-1))\n",
    "y_data_imputed = np.zeros((len(air_quality_data), prediction_horizon, 1))\n",
    "y_data_no_imputed = np.zeros((len(air_quality_data), prediction_horizon, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b86291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_imputed.shape ,X_data_no_imputed.shape ,y_data_imputed.shape ,y_data_no_imputed.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(cols):\n",
    "    for j in range(timesteps):\n",
    "        X_data_imputed[:, j, i] = air_quality_data[name].shift(timesteps - j - 1).fillna(method=\"bfill\")\n",
    "        X_data_no_imputed[:, j, i] = air_quality_data[name].shift(timesteps - j - 1)\n",
    "for j in range(prediction_horizon):\n",
    "    y_data_imputed[:, j, 0] = air_quality_data[TARGET].shift(prediction_horizon - j - 1).fillna(method=\"bfill\")\n",
    "    y_data_no_imputed[:, j, 0] = air_quality_data[TARGET].shift(prediction_horizon - j - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72444ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_imputed.shape ,X_data_no_imputed.shape ,y_data_imputed.shape ,y_data_no_imputed.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ef14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_imputed = X_data_imputed[timesteps:-prediction_horizon]\n",
    "X_data_no_imputed = X_data_no_imputed[timesteps:-prediction_horizon]\n",
    "y_data_imputed = y_data_imputed[timesteps:-prediction_horizon]\n",
    "y_data_no_imputed = y_data_no_imputed[timesteps:-prediction_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_imputed.shape ,X_data_no_imputed.shape ,y_data_imputed.shape,y_data_no_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af706",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_imputed[np.isnan(y_data_imputed)],  y_data_imputed[np.isinf(y_data_imputed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_no_imputed[np.isnan(y_data_no_imputed)],  y_data_no_imputed[np.isinf(y_data_no_imputed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c746d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_no_imputed[np.isnan(X_data_no_imputed)],  X_data_no_imputed[np.isinf(X_data_no_imputed)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904168b-6075-437a-bbdf-5d5912c1ee55",
   "metadata": {},
   "source": [
    "# Extract the last observation of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pats, seq, n_features = X_data_no_imputed.shape\n",
    "timeseries_last_obs_data = []\n",
    "for i in range(nb_pats):\n",
    "    Index_Last=(~np.isnan(X_data_no_imputed[i,:,:])).cumsum(0).argmax(0)\n",
    "    Last_Indices = np.reshape(Index_Last,(1,n_features))\n",
    "    Last_Values = np.take_along_axis(X_data_no_imputed[i,:,:], Last_Indices, axis = 0)\n",
    "    timeseries_last_obs_data.append(np.repeat(Last_Values, seq, axis=0))\n",
    "last_obs_data=np.stack(timeseries_last_obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs_data[np.isnan(last_obs_data)],  last_obs_data[np.isinf(last_obs_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_median = np.nanmedian(last_obs_data, axis=0)\n",
    "last_obs_data_imputed = np.where(np.isnan(last_obs_data), \n",
    "                         col_median, last_obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc38a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_obs_data[np.isnan(last_obs_data)],  last_obs_data[np.isinf(last_obs_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59564269",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs_data_imputed[np.isnan(last_obs_data_imputed)],  last_obs_data_imputed[np.isinf(last_obs_data_imputed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98697988",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs_data_imputed, last_obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079db2b6-bf09-4c8f-b5e3-319b6d8987a0",
   "metadata": {},
   "source": [
    "# The frequency observation of each feature -----> np.isfinite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99bb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnan\n",
    "freq_list= []\n",
    "nb_pats, seq, n_features = X_data_no_imputed.shape\n",
    "for i in range(nb_pats):\n",
    "    data_samples=  np.expand_dims(X_data_no_imputed[i,:,:], axis=0)\n",
    "    nan_counts = np.sum(np.isnan(data_samples), axis=(0, 1))\n",
    "    freq_list.append(np.repeat(np.expand_dims(nan_counts, axis=0), seq, axis=0))\n",
    "freqs = np.stack(freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec61e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import itertools  \n",
    "import datetime\n",
    "from collections import namedtuple, defaultdict\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_imputation(data):\n",
    "    store_index = []\n",
    "    for index in  range(len(data)):\n",
    "        try: \n",
    "            if len(data)>1:\n",
    "                store_index.append([data[index], data[index+1]])\n",
    "            else:\n",
    "                store_index.append(data[index])\n",
    "        except:\n",
    "            pass\n",
    "    return store_index\n",
    "def _forward_with_last_measured(data, hours_data=48):\n",
    "    cols_data = data.columns.to_list()\n",
    "    times_hours_data = np.arange(0,hours_data,1)\n",
    "    notna_cols_indexes, nan_cols_indexes  = defaultdict(list),defaultdict(list)\n",
    "    indexes_last_indicator = defaultdict(list)\n",
    "    for col in cols_data:\n",
    "        notna_cols_indexes[col].append(list(data.loc[pd.notna(data[col]), :].index))\n",
    "        nan_cols_indexes[col].append(list(data.loc[pd.isna(data[col]), :].index))\n",
    "        indexes_last_indicator[col].append([data[col].notna()[::-1].idxmax(),\n",
    "                                      data[col].isna()[::-1].idxmax()])\n",
    "    notna_cols_indexes_ = {key: list(itertools.chain.from_iterable(value)) \n",
    "                                 for key , value in notna_cols_indexes.items() if list(itertools.chain.from_iterable(value))}\n",
    "    notna_cols_indexes_ ={ key:value  for key , value in notna_cols_indexes_.items() if len(value)!=len(times_hours_data)}\n",
    "\n",
    "    nan_cols_indexes = {key: list(itertools.chain.from_iterable(value)) \n",
    "                                for key , value in nan_cols_indexes.items()}\n",
    "    nan_cols_indexes_ ={key:value  for key , value in nan_cols_indexes.items() \n",
    "                                if len(value)!=len(times_hours_data)}\n",
    "    nan_cols_indexes_ ={key:value  for key , value in nan_cols_indexes_.items() if value}\n",
    "    \n",
    "    indexes_last_indicator = {key: list(itertools.chain.from_iterable(value)) \n",
    "                                    for key , value in indexes_last_indicator.items()}\n",
    "    matrix_indexes_notna = {key:forward_imputation(value) for key, value in notna_cols_indexes_.items()}\n",
    "    matrix_notna_with_last = {key:list(itertools.chain.from_iterable([matrix_indexes_notna[key], [indexes_last_indicator[key]]])) \n",
    "                                 for key in indexes_last_indicator if key in matrix_indexes_notna }\n",
    "    final_matrix_indexes = defaultdict(list)\n",
    "    for key, vals in matrix_notna_with_last.items():\n",
    "        for val in vals:\n",
    "            if isinstance(val, list):\n",
    "                final_matrix_indexes[key].append(val)\n",
    "    matrix_range_indexes_cols = {key:[final_matrix_indexes[key], nan_cols_indexes_[key]] \n",
    "                                 for key in nan_cols_indexes_ if key in final_matrix_indexes}\n",
    "    \n",
    "    range_indexes_cols_imputed = {}\n",
    "    for key, value in matrix_range_indexes_cols.items():\n",
    "        range_values = [[notna_ind,nan] for nan in value[1] \n",
    "                      for notna_ind in value[0] if notna_ind[0]<=nan<=notna_ind[1]]\n",
    "        range_indexes_cols_imputed[key]=range_values\n",
    "    return range_indexes_cols_imputed\n",
    "\n",
    "def forward_with_last_measured_value_with_time_elasped_interval(data_copy, mask_data, hrs_used=48):\n",
    "    resultats = _forward_with_last_measured(data_copy, hours_data=hrs_used)\n",
    "    data =data_copy.copy()\n",
    "    mask_forward =mask_data.copy()\n",
    "    for key , values in resultats.items():\n",
    "        for k, group in groupby(values, lambda x:x[0]):\n",
    "            vals_ = list(itertools.chain.from_iterable(list(group)))\n",
    "            vals_ = np.array([val for val in vals_ if not isinstance(val, list)])\n",
    "            if k[1]<hrs_used-1:  \n",
    "                for index in np.arange(k[0]+1, k[1]):\n",
    "                    data.at[index, key]= data._get_value(k[0], key)\n",
    "                    mask_forward.at[index, key]= index-k[0]\n",
    "            else:\n",
    "                for index in np.arange(k[0]+1, k[1]+1):\n",
    "                    if pd.isnull(data._get_value(index, key)):\n",
    "                        #print(index, key, data[key])\n",
    "                        data.at[index, key]= data._get_value(k[0], key)\n",
    "                        mask_forward.at[index, key]= index-k[0]\n",
    "                    else:\n",
    "                        pass\n",
    "    return data, mask_forward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c0c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data_no_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dacd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_features = cols\n",
    "x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta, features_inputs= [],[]\n",
    "for x_data,freq in tqdm(zip(X_data_no_imputed, freqs), total=len(y_data_imputed),\n",
    "                            desc='Iterating over samples'):\n",
    "    masked_timeseries = np.where((pd.isnull(x_data)), np.nan, 0)\n",
    "    features_data = pd.DataFrame(x_data, columns=x_features)\n",
    "    masked_episode_timeseries = pd.DataFrame(masked_timeseries, columns=x_features)\n",
    "    data_for_imp,time_elasped_timeseries=forward_with_last_measured_value_with_time_elasped_interval(features_data,\n",
    "                                                                                                     masked_episode_timeseries,\n",
    "                                                                                                     hrs_used=timesteps)\n",
    "    timedelta.append(time_elasped_timeseries.values)\n",
    "    features_inputs.append(masked_episode_timeseries.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_time_data=np.stack(timedelta)\n",
    "mask_data=np.stack(features_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d00107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name =f\"BeijingAirQualityMultiSite_frequency_weighting_{timesteps}_timesteps\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn=f\"/data/{dataset_name}\"\n",
    "if not os.path.exists(dn):\n",
    "    os.makedirs(dn)\n",
    "dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f26fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(dn,f\"data_{timesteps}.npz\"),\n",
    "         last_obs=last_obs_data, \n",
    "         last_obs_imputed=last_obs_data_imputed, \n",
    "         targets_data=y_data_imputed,\n",
    "         frequencies_data=freqs, \n",
    "         timeseries_data=X_data_no_imputed,\n",
    "         timeseries_data_imputed=X_data_imputed,\n",
    "         timedelta_data=delta_time_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_time_data[np.isnan(delta_time_data)] = 999\n",
    "delta_time_data[np.isinf(delta_time_data)] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aef899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_obs_data[np.isnan(last_obs_data)],  last_obs_data[np.isinf(last_obs_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25002396",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_data_max = np.nanmax(X_data_no_imputed,axis=0)\n",
    "timeseries_data_min = np.nanmin(X_data_no_imputed, axis=0)\n",
    "\n",
    "last_observation_data_max = np.nanmax(last_obs_data,axis=0)\n",
    "last_observation_data_min = np.nanmin(last_obs_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = (X_data_no_imputed - timeseries_data_min) / (timeseries_data_max - timeseries_data_min)\n",
    "\n",
    "last_data = (last_obs_data - last_observation_data_min) / (last_observation_data_max - last_observation_data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ea215",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data[np.isnan(last_data)],  last_data[np.isinf(last_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data[np.isnan(features_data)],  features_data[np.isinf(features_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS =np.squeeze(y_data_imputed,axis=-1)\n",
    "TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_max = TARGETS.max()\n",
    "targets_min = TARGETS.min()\n",
    "\n",
    "targets = (TARGETS - targets_min) / (targets_max - targets_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ae84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders_double_cv_cohort(train_x, train_t,  train_last,  train_freq, train_y, \n",
    "                          test_x, test_x2, test_t,  test_last,  test_freq,  test_y, BATCH_SIZE=128):\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(train_x,dtype=torch.float32),\n",
    "                                  torch.tensor(train_t,dtype=torch.float32),\n",
    "                                  torch.tensor(train_last,dtype=torch.float32),\n",
    "                                  torch.tensor(train_freq,dtype=torch.float32),\n",
    "                                  torch.tensor(train_y,dtype=torch.float32))  \n",
    "    \n",
    "    \n",
    "    valid_dataset = TensorDataset(torch.tensor(test_x,dtype=torch.float32),\n",
    "                                  torch.tensor(test_t,dtype=torch.float32),\n",
    "                                  torch.tensor(test_last,dtype=torch.float32),\n",
    "                                  torch.tensor(test_freq,dtype=torch.float32),\n",
    "                                  torch.tensor(test_x2,dtype=torch.float32),\n",
    "                                  torch.tensor(test_y,dtype=torch.float32))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, valid_data_loader\n",
    "def cv_fold_splits_cohorts(data,data_real,time_data,last_data,features_freqs, target, n_fold=2):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    test_data_loader, training_data, validation_data = [], [], []\n",
    "    all_test_notes = []\n",
    "    kfold = KFold(n_splits=n_fold, shuffle=True, random_state=n_fold)\n",
    "    for index, (train_index, test_index) in enumerate(kfold.split(data, target)):\n",
    "        print('<-------OUTTER FOLD', index + 1, '------->')\n",
    "        \n",
    "        x_train, x_test = data[train_index], data[test_index]\n",
    "        print(data.shape,x_train.shape,x_test.shape)\n",
    "        _, x_test_real = data_real[train_index], data_real[test_index]\n",
    "        x_train_last, x_test_last = last_data[train_index], last_data[test_index]\n",
    "        x_train_freq, x_test_freq = features_freqs[train_index], features_freqs[test_index]\n",
    "        x_train_t, x_test_t = time_data[train_index], time_data[test_index]\n",
    "       \n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        _, test_loader =  dataloaders_double_cv_cohort(x_train, x_train_t,  x_train_last,\n",
    "                                                x_train_freq,  y_train, \n",
    "                                                x_test, x_test_real, x_test_t,   x_test_last, \n",
    "                                                x_test_freq,  y_test)\n",
    "        \n",
    "        X_train, X_valid, y_train_fold, y_valid_fold = train_test_split(list(zip(x_train,x_train_t,\n",
    "                                                                                 x_train_last,x_train_freq,\n",
    "                                                                                 )), y_train, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=n_fold)\n",
    "        #Training data\n",
    "        x_train_in =np.array([X_train[i][0] for i in range(len(X_train))])\n",
    "        x_train_in_t= np.array([X_train[i][1] for i in range(len(X_train))])\n",
    "        x_train_in_last= np.array([X_train[i][2] for i in range(len(X_train))])\n",
    "        x_train_in_freq= np.array([X_train[i][3] for i in range(len(X_train))])\n",
    "        # Validation data\n",
    "        x_val_in = np.array([X_valid[i][0] for i in range(len(X_valid))])\n",
    "        x_val_in_t = np.array([X_valid[i][1] for i in range(len(X_valid))])\n",
    "        x_val_in_last = np.array([X_valid[i][2] for i in range(len(X_valid))])\n",
    "        x_val_in_freq = np.array([X_valid[i][3] for i in range(len(X_valid))])\n",
    "        #print(x_val_in_adm_t.shape, x_val_in_adm_t)\n",
    "        train_loader, val_loader =  dataloaders_double_cv_cohort(x_train_in, x_train_in_t, x_train_in_last, \n",
    "                                                          x_train_in_freq,  y_train_fold ,\n",
    "                                                          \n",
    "                                                          x_val_in,x_val_in, x_val_in_t,x_val_in_last,\n",
    "                                                          x_val_in_freq, y_valid_fold)\n",
    "        \n",
    "        training_data.append(train_loader)\n",
    "        validation_data.append(val_loader)\n",
    "        \n",
    "        test_data_loader.append(test_loader)\n",
    "    return test_data_loader, training_data, validation_data, x_train_in.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5822688-fcb5-41b1-856c-b0ad0a30dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = (TARGETS - targets_min) / (targets_max - targets_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582347f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader, train_loader, valid_loader,data_shape = cv_fold_splits_cohorts(features_data,\n",
    "                                                                            X_data_no_imputed,   \n",
    "                                                                            delta_time_data,\n",
    "                                                                            last_data, \n",
    "                                                                            freqs,targets,\n",
    "                                                                            n_fold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188752f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = features_data.shape[1]\n",
    "input_dim = features_data.shape[-1]\n",
    "hidden_dim, output_dim  = 64, 1\n",
    "seq_length, input_dim, hidden_dim, output_dim, data_shape\n",
    "seq_length, input_dim, hidden_dim, output_dim, data_shape\n",
    "taskname=f\"BeijingAirQuality_{seq_length}_HRS_DATA_128\".upper()\n",
    "task=f\"{os.path.join(dn, f'{taskname}')}\"\n",
    "if not os.path.exists(task):\n",
    "    os.makedirs(task)\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data[np.isnan(features_data)],  features_data[np.isinf(features_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99daea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loader = random.sample(list(zip(train_loader, valid_loader)), len(train_loader))\n",
    "np.savez(os.path.join(task, f\"train_test_data.npz\"), \n",
    "            folds_data_test= test_loader,\n",
    "            folds_data_train_valid= train_val_loader,)\n",
    "        \n",
    "np.savez(os.path.join(task, f\"data_max_min.npz\"), \n",
    "         data_max=timeseries_data_max, data_min=timeseries_data_min,\n",
    "         data_targets_max=targets_max, data_targets_min=targets_min,\n",
    "         shape_data=data_shape,seq_length=seq_length,\n",
    "         input_dim=input_dim)\n",
    "input_dim"
   ]
  },
}
